{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/philastotle/dissertation-pointnet/blob/master/PointNetBasic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIspKaMirwJS"
   },
   "source": [
    "# Prediction of radiotherapy plan violation from spatial arrangement of target and organ at risk structures using deep learning\n",
    "\n",
    "_By Phillip Hungerford,  University of New South Wales_\n",
    "\n",
    "## PointNet Basic Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXQrflMUTLhf"
   },
   "source": [
    "### 3.1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1q1g6uXG47wi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# To time \n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seeds=42\n",
    "from numpy.random import seed\n",
    "import random\n",
    "random.seed(seeds)\n",
    "seed(seeds)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(seeds)\n",
    "\n",
    "## For model\n",
    "# Import dependencies\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, MaxPooling1D, Convolution1D, Dropout, Flatten, BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "# Evaluation dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AUC\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape is:  (182, 3140, 4)\n",
      "Validation shape is:  (46, 3140, 4)\n",
      "Test shape is:  (58, 3140, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load 3140 data that excludes the body\n",
    "X = np.load('../2_pipeline/3140-xyzl.npy')\n",
    "y = np.load('../2_pipeline/labels.npy')\n",
    "\n",
    "#split data into 1: train+validation set and 2: test set \n",
    "X_train_val, X_test, y_train_val, y_test = \\\n",
    "train_test_split(X, y, random_state=0, test_size=0.2)\n",
    "\n",
    "# split train+validation set into 1a) training and 1b) validation sets\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "train_test_split(X_train_val, y_train_val, random_state=1, test_size=0.2)\n",
    "\n",
    "#from keras.utils import to_categorical\n",
    "#y_test = to_categorical(y_test)\n",
    "#y_train = to_categorical(y_train)\n",
    "\n",
    "print('Training shape is: ', X_train.shape)\n",
    "print('Validation shape is: ', X_val.shape)\n",
    "print('Test shape is: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points=3140\n",
    "# Hyperparameters \n",
    "max_epochs=200\n",
    "batch_size=32\n",
    "dropout_rate = 0.5\n",
    "opt = 'adam'\n",
    "#opt = SGD(lr=0.0001, momentum=0.9)\n",
    "\n",
    "# Class weights\n",
    "class_weight = {0: 0.2,\n",
    "                1: 0.8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xEya6kiCVAY2"
   },
   "source": [
    "# PointNet Basic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DY3DUvTctfmy"
   },
   "source": [
    "## Model 2: Extra-column (n,1024,4) -> (x,y,z,l)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1527
    },
    "colab_type": "code",
    "id": "iCtjaz87F9v7",
    "outputId": "fe5668a3-0105-4451-c3a1-a3cee342f986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3140, 64)          320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3140, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 3140, 64)          4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 3140, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 3140, 64)          4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3140, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 3140, 128)         8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3140, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 3140, 1024)        132096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 3140, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 512)            524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1, 512)            2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 256)            131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1, 256)            1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1, 1)              257       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 813,889\n",
      "Trainable params: 809,665\n",
      "Non-trainable params: 4,224\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-00e30dc856fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m history = model.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs,\\\n\u001b[1;32m     37\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                     class_weight=class_weight)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Evaluate Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "### POINTNET ARCHITECTURE\n",
    "################################################################################\n",
    "# Point functions (MLP implemented as conv1d)\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(64, 1, input_shape=(num_points, 4), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution1D(64, 1, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution1D(64, 1, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution1D(128, 1, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution1D(1024, 1, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Symmetric function: max pooling\n",
    "model.add(MaxPooling1D(pool_size=num_points))\n",
    "\n",
    "#fully connected\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(rate=dropout_rate))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(rate=dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.add(Flatten())\n",
    "\n",
    " # MLP on global point cloud vector\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "################################################################################\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs,\\\n",
    "                    shuffle=True, verbose=0, validation_data=(X_val, y_val),\\\n",
    "                    class_weight=class_weight)\n",
    "################################################################################\n",
    "# Evaluate Model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Average loss of: \", scores[0])\n",
    "print(\"Average accuracy of: \", scores[1])\n",
    "\n",
    "################################################################################\n",
    "# Classification Report\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    " \n",
    "# show a nicely formatted classification report\n",
    "print(\"[INFO] evaluating network...\")\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred.round()))\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# plot loss during training\n",
    "plt.subplot(211)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "################################################################################\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print(\"\\n###################### Model Performance ############################\")\n",
    "print(\"\\n#####################################################################\")\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('\\nTrain: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "print(\"\\n#####################################################################\")\n",
    "################################################################################\n",
    "import matplotlib.pyplot as plt\n",
    "# plot loss during training\n",
    "plt.subplot(211)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "################################################################################\n",
    "print(\"\\n#####################################################################\")\n",
    "# Create the confusion matrix\n",
    "ann_cm = confusion_matrix(y_true = y_test, y_pred = y_pred.round())\n",
    "print(\"\\nOur test confusion matrix yields: \")\n",
    "print(ann_cm)\n",
    "print(\"\\n#####################################################################\")\n",
    "\n",
    "#Classification report\n",
    "ann_report = classification_report(y_test, y_pred.round())\n",
    "print(\"\\nClassfication Report for test:\\n\", ann_report)\n",
    "print(\"\\n#####################################################################\")\n",
    "\n",
    "#Calculate AUC score\n",
    "ann_auc = roc_auc_score(y_test, y_pred.round())\n",
    "print(\"\\nOur testing AUC for ann is: \", ann_auc)\n",
    "\n",
    "# Calculate false positive and true positive rates\n",
    "fpr_ann, tpr_ann, thresholds_ann = roc_curve(y_test, y_pred.round())\n",
    "\n",
    "# Plot AUC \n",
    "plt.figure()\n",
    "plt.plot(fpr_ann, tpr_ann, color='purple', lw=2, label='ANN (area = {:.3f})'.format(ann_auc))\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "print(\"\\n#####################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save this model? (y/n) n\n"
     ]
    }
   ],
   "source": [
    "def model_save(modelname):\n",
    "    answer = input(\"Do you want to save this model? (y/n) \")\n",
    "    if answer == 'y':\n",
    "        #Save history\n",
    "        with open('/trainHistoryDict', 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)\n",
    "            \n",
    "        # Save model\n",
    "        from keras.models import load_model\n",
    "        model.save( modelname +'.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "        \n",
    "model_save('tester_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KH05_bwo_vko"
   },
   "source": [
    "K FOLDS Cross validated score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qer4WIVj_vVS"
   },
   "outputs": [],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=61)\n",
    "ann_cv_scores = []\n",
    "\n",
    "#Run loop for manual 5-fold cross validation\n",
    "for train, test in kfold.split(X, y):\n",
    "    #scale data\n",
    "    scaler = StandardScaler()\n",
    "    #fit on training\n",
    "    scaler.fit(X.iloc[train])\n",
    "    #build model with optimized parameters\n",
    "    model = Sequential()\n",
    "    model.add(Dense(H1, input_dim=8, activation='selu')) #use optimized hidden layer size\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy']) #with optimized optimizers\n",
    "    # Fit the model\n",
    "    model.fit(scaler.transform(X.iloc[train]), y[train], epochs=150, batch_size=10, verbose=0)\n",
    "    y_pred_ann = model.predict(scaler.transform(X.iloc[test]))\n",
    "    scores = roc_auc_score(y[test], y_pred_ann)\n",
    "    ann_cv_scores.append(scores)\n",
    "\n",
    "ann_auc_cv = numpy.mean(ann_cv_scores)\n",
    "print(\"ANN CV score: \", ann_auc_cv)\n",
    "\n",
    "print(\"\\n#####################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z0AzPtXd_-YF"
   },
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0LOAUBi-pra"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the Pipeline we will use\n",
    "Scaler = StandardScaler()\n",
    "Log_Reg = LogisticRegression(penalty = 'l2')\n",
    "\n",
    "pipe = Pipeline([('Transform', Scaler), ('Estimator', Log_Reg)])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'Estimator__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'Estimator__class_weight': [{0:0.1, 1:0.9}, {0:0.2, 1:0.8}, \\\n",
    "                                          {0:0.3, 1:0.7}, {0:0.4, 1:0.6}, \\\n",
    "                                          {0:0.5, 1:0.5}]}\n",
    "              \n",
    "\n",
    "print(\"Parameter grid:\")\n",
    "print(\"class_weight: {}\".format(param_grid['Estimator__class_weight']))\n",
    "print(\"C: {}\".format(param_grid['Estimator__C']))\n",
    "\n",
    "# Now initialise the GridSearchCV class by passing it the pipeline we have created\n",
    "grid_search=GridSearchCV(pipe, param_grid=param_grid, cv=5, scoring='f1_macro')\n",
    "\n",
    "# We stratify in order to have the same number of classes in the different splits.\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y_binary, random_state=0, stratify=y_binary, test_size=0.2)\n",
    "\n",
    "## Find the best parameters\n",
    "# It takes a while to run ...\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation f1 score: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "## Visualise grid results\n",
    "import mglearn\n",
    "import warnings; warnings.simplefilter('ignore') #prevent warnings\n",
    "\n",
    "# convert results to DataFrame\n",
    "results = pd.DataFrame(grid_search.cv_results_) \n",
    "# show the first few rows \n",
    "display(results[:][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiIX-EMFpWxg"
   },
   "outputs": [],
   "source": [
    "# Load model.\n",
    "#model = load_model('my_model10204D.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cvm8Ri02pJUs"
   },
   "source": [
    "## 99. Cross Validation beta\n",
    "Note using different activation and loss functions as opposed to previous models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "phUG2LIVpI8V",
    "outputId": "f22b05cf-b861-431c-951d-091c0c32d90e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 69.44%\n",
      "acc: 69.44%\n",
      "acc: 69.01%\n",
      "acc: 30.99%\n",
      "59.72% (+/- 16.59%)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "\n",
    "# Load 1024 data \n",
    "X = np.load('../2_pipeline/1020-4D.npy')\n",
    "y = np.load('../2_pipeline/labels.npy')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X, y):\n",
    "    ### POINTNET ARCHITECTURE\n",
    "    \n",
    "    # Point functions (MLP implemented as conv2d\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(64, 1, input_shape=(num_points, 3, 3), activation='relu'))\n",
    "    model.add(Convolution2D(64, 1, activation='relu'))\n",
    "    model.add(Convolution2D(64, 1, activation='relu'))\n",
    "    model.add(Convolution2D(128, 1, activation='relu'))\n",
    "    model.add(Convolution2D(1024, 1, activation='relu'))\n",
    "\n",
    "    # Symmetric function: max pooling\n",
    "    model.add(MaxPooling2D(pool_size=(num_points,3), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "    #fully connected\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Compile model\n",
    "    # MLP on global point cloud vector\n",
    "    model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gz3XKQVrfL2m"
   },
   "source": [
    "The model with 1024 points scored:\n",
    "* Test loss: 0.8002310043887089\n",
    "* Test accuracy: 0.631578951132925\n",
    "\n",
    "The model with body and 4096 points scored:\n",
    "* Test loss:  1.2039089792653135\n",
    "* Test accuracy:  0.6210526353434512\n",
    "\n",
    "The model without the body with 4096 points scored:\n",
    "* Test loss:  0.9736735845866956\n",
    "* Test accuracy:  0.5894736785637705\n",
    "\n",
    "The model without the body and 1024 points scored:\n",
    "* Test loss: x\n",
    "* Test accuracy: x\n",
    "\n",
    "Model with 4096 points scored:\n",
    "* Test loss of: 1.2039089792653135\n",
    "* Test accuracy of: 0.6210526353434512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67XEOdVGIldY"
   },
   "source": [
    "## Apply a grid search for hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufKyP95IIk94"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "PointNetBasic.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
